{
    "contents" : "---\ntitle: \"Simulated Data Work\"\nauthor: \"Jonathan Waring\"\ndate: \"July 1, 2016\"\noutput: pdf_document\ncsl: plos-biology.csl\nbibliography: References.bib\n---\n\n```{r allLibraries, echo=FALSE, message=FALSE}\nrequire(pomp)\nrequire(ggplot2)\nrequire(magrittr)\nrequire(foreach)\nrequire(iterators)\nrequire(doParallel)\nrequire(plyr)\nrequire(knitcitations)\nrequire(bibtex)\n```\n\n#Simulated Data Work for REU Summer 2016 Project \n\n##Project Description \n\nDuring an outbreak of an emerging infection, there are many epidemiologically important quantities that we need to estimate in order to get a better picture of how severe the epidemic may turn out to be.  These include quantities such as the mean infectious period and the transmission potential of the pathogen.  Fitting transmission models to incidence reports has become a standard way of achieving quick estimates of these parameters.  Too often, practitioners use cumulative data (total number of infections to date) rather than raw incidence (number of new cases in a defined reporting period).  There is evidence to suggest this choice can severely affect our perception of the variability in parameters and hence the certainty in predictions.  This project is focused on further elaborating on this problem using simulated epidemic data. The aim is to use both deterministic and stochastic models to fit to both raw and cumulative data in order to systematically assess the likely biases and errors that result from the choice of data.\n\n##POMP Model\n\npomp [@King_2016a] is a package in the R progamming language that encodes a partially-observed Makrov process model with a time series. One implements the model by specififying different components written as R functions or C code snippets. The code below demonstrates how the epidemic time-series data used in this project was created (Simulator.R), trajectory matched, and filtered (IF2 algorithm). The pomp model described below is used in most of the scripts to describe the data. \n\nThe continuous-time process model (simulation of state process given parameters) was specified as: \n```{r Continuous-time process model}\n#Continuous-time process model (rprocess)\nstep.fun<- Csnippet(\"\n                    double dQ = rgammawn(beta_sd,dt);\n                    double lambda = Beta*I/N*dQ/dt;\n                    double births = rpois(mu*N);\n                    \n                    S += (births - lambda*S - mu*S)*dt;\n                    I += (lambda*S - gamma*I - mu*I)*dt;\n                    RealCases += gamma*I*dt;\n                    \n                    \")\n```\n\nThe model was initialized as follows: \n```{r Initializer}\n#Initializer\ninit <- Csnippet(\"\n                  S = N-1;\n                  I= 1;\n                  RealCases = 1;\n                  \")\n```\n\nThe measurement process was modeled as the following rmeasure (simulation of observation process given state and parameters) and dmeasure (evaluation of the likelihood of a set of observations given the states and parameters): \n```{r Measurement process}\n#Rmeasure model\nrmeas <- Csnippet(\"\n                  Cases = rpois(RealCases * rho);                \n                  \")\n  \n#Dmeasure model\ndmeas <- Csnippet(\"\n                  lik = dpois(Cases, RealCases * rho, give_log);                  \n                  \")\n```\n\nThe deterministic skeleton as a point in state space, given parameters was evaluated by: \n```{r Skeleton}\n#Deterministic model skeleton\nskel <- Csnippet(\"\n                  double lambda = Beta*I/N;\n                  double births = mu*N;\n                   \n                  DS = births - lambda*S -mu*S;\n                  DI = lambda*S -gamma*I -mu*I;\n                  DRealCases = gamma*I;              \n                  \")\n```\n\nLastly, the paramters were transformed using:\n```{r Param trans}\n#Paramter transformations\nlogtrans <- Csnippet(\"\n                      Tbeta_sd = log(beta_sd);\n                      TBeta = log(Beta);\n                      Tmu = log(mu);\n                      Tgamma = log(gamma);\n                      Trho = log(rho);\n                      TN = log(N);                     \n                      \")\n  \nexptrans <- Csnippet(\"\n                      Tbeta_sd = exp(beta_sd);\n                      TBeta = exp(Beta);\n                      Tmu = exp(mu);\n                      Tgamma = exp(gamma);\n                      Trho = exp(rho);\n                      TN = exp(N);                     \n                      \")\n```\n\nTo construct the pomp model, you use the constructor like such:\n```{r pomp}\nsampleData <- read.csv(file =\"sample_data.csv\")\nsampleData <- subset(sampleData, select=c(\"time\", \"Cases\"))\nmodel <- pomp(sampleData, time=\"time\", t0=0, \n              rprocess=euler.sim(step.fun, delta.t=0.1), initializer = init,\n              statenames=c(\"S\", \"I\", \"RealCases\"), paramnames=c(\"Beta\", \"gamma\", \"mu\", \n              \"rho\", \"N\",\"beta_sd\"), obsnames = \"Cases\", zeronames = \"RealCases\", \n              rmeasure = rmeas, dmeasure = dmeas, skeleton = vectorfield(skel), \n              toEstimationScale=logtrans, fromEstimationScale = exptrans)\n```\n\n##Simulator\n\n25 simulated epidemic time-series data sets were generated using the pomp model above. Parameters for the model were set to the following levels: beta_sd = 0.1, Beta=1.5, mu=0.1, gamma=0.6, rho=0.8, N=10000. The example code is below. \n\n```{r simulate}\nS.0 <- 1e4-1\nI.0 <- 1\nN <- S.0+I.0\nnewSimulation <- simulate(model,nsim=1,params=c(beta_sd= 0.1, Beta = 1.5, mu=0.1, gamma=0.6, \n                                                rho= 0.8, N=N, S.0= S.0, I.0 = I.0, \n                                                RealCases.0 = 1),as.data.frame=T)\n```\n\n\n\nThis will produce something data that looks similar to this:\n\n```{r plot, echo=FALSE}\nggplot(data=newSimulation, mapping=aes(x=time, y = Cases)) +\n  geom_line()+geom_point()+expand_limits(y=0)+theme_classic()\n```\n\nThe Simulator.R script will generate a list of allSims, which contains 24 simulated data frames for epidemic data generated from the pomp model. It will be saved as a file called: simulations.Rda. The first simulation is in a file called sample_data.csv. \n\n##Trajectory Matching \n\nTrajectory matching is the method of fitting a deterministic model to data assuming independent errors. In pomp, the function traj.match searches parameter space to find parameters under which the likelihood of the data, given a trajectory of the deterministic skeleton, is maximized.\n\nFor each of the 25 simulations, the RawSimDataTrajMatch.R and CumSimDataTrajMatch.R scripts generates 38 trajectory matches for the 25 simulations. The 38 different trajectory matches each uses a different amount of time points, in order to see if the amount of points used changes which data choice (raw or cumulative) is more effective. \n\nThe results are saved in allCumTrajMatches.Rda and allRawTrajMatches.Rda files as ggplots. These scripts can be edited to save them as something other than the plots. \n\nUsing the above pomp model, you can do a raw trajectory match with the following code. \n\n```{r rawTrajmatch}\ntm <- traj.match(model, start=c(beta_sd=0.12, Beta=1.8, mu=0.15, gamma=0.65, rho=0.75, N=10000), \n                     est=c(\"beta_sd\", \"Beta\", \"mu\", \"gamma\", \"rho\", \"N\"), transform = TRUE)\n```\n\nThen you can run simulations to see how the fitted model compares to the data. \n```{r plotting}\np <- simulate(tm,nsim=10,as.data.frame=TRUE,include.data=TRUE)\np <- subset(p, select=c(\"time\", \"Cases\", \"sim\"))\np <- subset(p, !is.na(p$Cases))\nggplot(data=p,aes(x=time,y=Cases,group=sim,alpha=(sim==\"data\")))+\n  scale_alpha_manual(name=\"\",values=c(`TRUE`=1,`FALSE`=0.2),\n                    labels=c(`FALSE`=\"simulation\",`TRUE`=\"data\"))+\n  geom_line()\n```\n\nCumulative trajectory matching can be done in a similar manner, as shown here:\n```{r cumTrajMatch}\ngamma <- 0.6\ncumulativeSampleData <- mutate(sampleData, Cases=trunc(cumsum(Cases-(gamma*Cases))))\ncumulativeModel <- pomp(cumulativeSampleData, time=\"time\", t0=0, \n                   rprocess=euler.sim(step.fun, delta.t=0.1), initializer = init,\n                   statenames=c(\"S\", \"I\", \"RealCases\"), paramnames=c(\"Beta\", \"gamma\", \"mu\", \"rho\", \"N\",\"beta_sd\"),\n                   obsnames = \"Cases\", zeronames = \"RealCases\", rmeasure = rmeas, dmeasure = dmeas, \n                   skeleton = vectorfield(skel), toEstimationScale=logtrans, fromEstimationScale = exptrans)\ncumulativeTM <- traj.match(cumulativeModel, start=c(beta_sd=0.12, Beta=1.8, mu=0.15, gamma=0.65, rho=0.75, N=10000), \n                     est=c(\"beta_sd\", \"Beta\", \"mu\", \"gamma\", \"rho\", \"N\"), transform = TRUE)\np <- simulate(cumulativeTM,nsim=10,as.data.frame=TRUE,include.data=TRUE)\np <- subset(p, select=c(\"time\", \"Cases\", \"sim\"))\np <- subset(p, !is.na(p$Cases))\nggplot(data=p,aes(x=time,y=Cases,group=sim,alpha=(sim==\"data\")))+\n  scale_alpha_manual(name=\"\",values=c(`TRUE`=1,`FALSE`=0.2),\n                    labels=c(`FALSE`=\"simulation\",`TRUE`=\"data\"))+\n  geom_line()\n```\n\n##Traj Match Analysis\n\n38 trajectory matches for Raw and Cumulative data was done for each of the 25 realizations using a different amount of time points (increment by 1 each time). All trajectory matches were done with starting estimates as follows: beta_sd=0.12, Beta=1.8, mu=0.15, gamma=0.65, rho=0.75, N=10000. Analysis of the trajectory matches was done in the Visuals.R script which plotted each of the 25 simulations together at the 38 different timestamps for both raw and cumulative data. An example is shown here with raw data and cumulative data with 23 points. \n\n![Figure 1: 25 Raw Simulated Epidemic trajectory matches at 23 timestamps](23raw.png)\n\n\\newpage\n\n![Figure 2: 25 Cumulative Simulated Epidemic trajectory matches at 23 timestamps](23cum.png)\n\nTrajectory matching showed similar results to that of the \"Avoidable errors in the modelling of outbreaks of emerging pathogens, with special references to Ebola\" paper [@King_2015]. When fitting the raw and cumulative incidence data to a deterministic model, the cumulative incidence curve superficially appears to do better during the epidemic takeoff. However, this is known to not be a robust estimate, and the uncertainity is not accurately quantified. Fitting cumulative data given the assumptions of the statistical error model has been shown to be problematic in past modeling research [@Grad_2012]. As more points are added to the data, the trajectory matching starts to fit better with raw incidence data, while still seeming to accurately quantify uncertainity. The cumulative data fits start to either slow down faster than the actual data or start to fail to takeoff at the beginning of the epidemic. To see this, run the Visuals.R script. It seems that with trajectory matching, raw data still seems to be the appropriate choice for the whole course of an epidemic.  \n\n##Iterated Filtering\n\nAnother method for maximizing likelihood of parameters of a partially-observed Markov process is iterated filtering. The iterated filtering algorithm [@Ionides_2015] runs a particle filter (sequential Monte Carlo algorithm) at each iteration on a perturbed version of the model, which effectively smooths the likelihood surface. Using our stochastic model, the RawIteratedFiltering.R and CumIteratedFiltering.R scripts run the iterated filtering algorithm over one instance of the 25 simulated epidemic time-series data for both Raw and Cumulative data. \n\n###Raw Iterated Filtering\n\nThe raw iterated filtering algorithm is defined here:\n```{r rawIteratedFiltering}\nregisterDoParallel()\nbake(file=\"RawFakeData-mif.rds\", {\n  guesses <- sobolDesign(lower=c(beta_sd=0.05, Beta=1.0, mu=0.01, gamma=0.1, rho=0.05, N=10000),\n                         upper=c(beta_sd=0.5, Beta=3.0, mu=1.0, gamma=0.99, rho=0.99, N=10000),\n                         nseq=100)\n  foreach (guess=iter(guesses,\"row\"),.combine=rbind,\n         .options.mpi=list(seed=334065675),\n         .packages=c(\"pomp\",\"magrittr\"),.errorhandling=\"remove\") %dopar% {\n           sample %>%\n             mif2(start=unlist(guess),Nmif=50,Np=1000,transform=TRUE,\n                  cooling.fraction.50=0.8,cooling.type=\"geometric\",\n                  rw.sd=rw.sd(beta_sd=0.02, Beta=0.02, mu=0.02, gamma=0.02, rho=0.02, N=0.02)) %>%\n             mif2() -> mf\n           ll <- logmeanexp(replicate(5,logLik(pfilter(mf))),se=TRUE)\n           data.frame(loglik=ll[1],loglik.se=ll[2],as.list(coef(mf)))\n         }\n}) -> mles\n```\nA useful way to view the data is with a pairwise scatterplot matrix:\n```{r matrix}\npairs(~loglik+beta_sd+Beta+mu+gamma+rho+N,data=mles)\n```\n\nFrom looking at the matrix, there does not seem to be any linear correlations between the parameters in this model. Most of the paramters seem to gravitate near one location for the best log likelihood (except for beta_sd and N). \n\nTo get better estimates and obtain likelihood-ratio-test based confidence intervals, I've constructed profile likelihoods for two important parameters: beta (transmission term) and gamma (recovery rate). In a likelihood profile, one varies the focal parameter across some range, maximizing the likelihood at each value of the focal parameter over the remaining parameters. \n\n####Raw Beta Profile \n\nThe beta profile is constructed below. \n```{r betaRaw profile}\nprofileDesign(\n  Beta=seq(from=1.0, to=3.0, length=20),\n  lower=c(beta_sd=0.05, mu=0.01, gamma=0.1, rho=0.05, N=10000),\n  upper=c(beta_sd=0.5, mu=1.0, gamma=0.99, rho=0.99, N=10000),\n  nprof=50\n) -> betaPD\n\nbake(\"RawFakedata_Beta-profile.rds\",{\n  foreach (p=iter(betaPD,\"row\"),\n           .combine=rbind,\n           .errorhandling=\"remove\",\n           .packages=c(\"pomp\",\"magrittr\",\"reshape2\",\"plyr\"),\n           .inorder=FALSE,\n           .options.mpi=list(seed=1680158025)\n  ) %dopar% {\n    sample %>% \n      mif2(start=unlist(p),Nmif=50,Np=1000,transform=TRUE,\n           cooling.fraction.50=0.8,cooling.type=\"geometric\",\n           rw.sd=rw.sd(beta_sd=0.02, mu=0.02, gamma=0.02, rho=0.02, N=0.02)) %>%\n      mif2() -> mf\n    \n    pf <- replicate(5,pfilter(mf,Np=1000))  ## independent particle filters\n    ll <- sapply(pf,logLik)\n    ll <- logmeanexp(ll,se=TRUE)\n    nfail <- sapply(pf,getElement,\"nfail\")  ## number of filtering failures\n    \n    data.frame(as.list(coef(mf)),\n               loglik = ll[1],\n               loglik.se = ll[2],\n               nfail.min = min(nfail),\n               nfail.max = max(nfail))\n  } %>% arrange(Beta,-loglik)\n}) -> beta_prof\n```\n\nPairwise scatterplot matrix for Beta: \n```{r betaSM}\npairs(~loglik+Beta+beta_sd+mu+gamma+rho+N,data=beta_prof,subset=loglik>max(loglik)-10)\n```\n\nThere appears to be a positive linear correlation between Beta and gamma, as well as Beta and N and Beta and mu in this profile design. \n\n```{r betaCIProfileGraph}\nbeta_prof %>% \n  mutate(Beta=signif(Beta,8)) %>%\n  ddply(~Beta,subset,loglik==max(loglik)) %>%\n  ggplot(aes(x=Beta,y=loglik))+geom_point()+geom_smooth()\n```\n\nThis graph shows Beta plotted as a function of log likeihood. The maximum log likelihood appears to be close to 1.5, which is the true parameter value. The gray ribbon surrounding the blue line represents the confidence interval estimate, and it is seen that 65% of the points actually fall within the 95% confidence interval. \n\nNow let us plot the data and 10 simulated realizations of the model process on the same axes.\n\n```{r betaProfileFit}\nbeta_prof %>% \n  subset(loglik==max(loglik)) %>% unlist() -> mle\nsimulate(model,params=mle,nsim=10,as.data.frame=TRUE,include.data=TRUE) %>%\n  ggplot(mapping=aes(x=time,y=Cases,group=sim,alpha=(sim==\"data\")))+\n  scale_alpha_manual(name=\"\",values=c(`TRUE`=1,`FALSE`=0.2),\n                     labels=c(`FALSE`=\"simulation\",`TRUE`=\"data\"))+\n  geom_line()\n```\n\n####Raw Gamma Profile\nThe gamma profile is constructed below. \n```{r gammaRaw profile}\nprofileDesign(\n  gamma=seq(from=0.1, to=1.0, length=20),\n  lower=c(beta_sd=0.05, mu=0.01, Beta=1.0, rho=0.05, N=10000),\n  upper=c(beta_sd=0.5, mu=1.0, Beta=3.0, rho=0.99, N=10000),\n  nprof=50\n) -> gammaPD\n\nbake(\"RawFakedata_gamma-profile.rds\",{\n  foreach (p=iter(gammaPD,\"row\"),\n           .combine=rbind,\n           .errorhandling=\"remove\",\n           .packages=c(\"pomp\",\"magrittr\",\"reshape2\",\"plyr\"),\n           .inorder=FALSE,\n           .options.mpi=list(seed=1680158025)\n  ) %dopar% {\n    sample %>% \n      mif2(start=unlist(p),Nmif=50,Np=1000,transform=TRUE,\n           cooling.fraction.50=0.8,cooling.type=\"geometric\",\n           rw.sd=rw.sd(beta_sd=0.02, mu=0.02, Beta=0.02, rho=0.02, N=0.02)) %>%\n      mif2() -> mf\n    \n    pf <- replicate(5,pfilter(mf,Np=1000))  ## independent particle filters\n    ll <- sapply(pf,logLik)\n    ll <- logmeanexp(ll,se=TRUE)\n    nfail <- sapply(pf,getElement,\"nfail\")  ## number of filtering failures\n    \n    data.frame(as.list(coef(mf)),\n               loglik = ll[1],\n               loglik.se = ll[2],\n               nfail.min = min(nfail),\n               nfail.max = max(nfail))\n  } %>% arrange(gamma,-loglik)\n}) -> gamma_prof\n```\n\nPairwise scatterplot matrix for Gamam: \n```{r gammaSM}\npairs(~loglik+gamma+beta_sd+mu+Beta+rho+N,data=gamma_prof,subset=loglik>max(loglik)-10)\n```\n\nGamma also apperas to have a positive linear correlation with beta, mu, and N. \n\n```{r gammaCIProfileGraph}\ngamma_prof %>% \n  mutate(gamma=signif(gamma,8)) %>%\n  ddply(~gamma,subset,loglik==max(loglik)) %>%\n  ggplot(aes(x=gamma,y=loglik))+geom_point()+geom_smooth()\n```\n\nThis graph shows gamma plotted as a function of log likeihood. The maximum log likelihood appears to be between the range of 0.5-1.0, with 0.6 being its true parameter value. The gray ribbon surrounding the blue line represents the confidence interval estimate, and it is seen that 90% of the points actually fall within the 95% confidence interval. Likely higher coverage due to a wider CI.  \n\nNow let us plot the data and 10 simulated realizations of the model process on the same axes.\n```{r gammaProfileFit}\ngamma_prof %>% \n  subset(loglik==max(loglik)) %>% unlist() -> mle\nsimulate(model,params=mle,nsim=10,as.data.frame=TRUE,include.data=TRUE) %>%\n  ggplot(mapping=aes(x=time,y=Cases,group=sim,alpha=(sim==\"data\")))+\n  scale_alpha_manual(name=\"\",values=c(`TRUE`=1,`FALSE`=0.2),\n                     labels=c(`FALSE`=\"simulation\",`TRUE`=\"data\"))+\n  geom_line()\n```\n\n###Cumulative Iterated Filtering\n\nThe cumulative iterated filtering algorithm is defined here:\n```{r cumIteratedFiltering}\nregisterDoParallel()\nbake(file=\"CumFakeData-mif.rds\", {\n  guesses <- sobolDesign(lower=c(beta_sd=0.05, Beta=1.0, mu=0.01, gamma=0.1, rho=0.05, N=10000),\n                         upper=c(beta_sd=0.5, Beta=3.0, mu=1.0, gamma=0.99, rho=0.99, N=10000),\n                         nseq=100)\n  foreach (guess=iter(guesses,\"row\"),.combine=rbind,\n           .options.mpi=list(seed=334065675),\n           .packages=c(\"pomp\",\"magrittr\"),.errorhandling=\"remove\") %dopar% {\n             sample %>%\n               mif2(start=unlist(guess),Nmif=50,Np=1000,transform=TRUE,\n                    cooling.fraction.50=0.8,cooling.type=\"geometric\",\n                    rw.sd=rw.sd(beta_sd=0.02, Beta=0.02, mu=0.02, gamma=0.02, rho=0.02, N=0.02)) %>%\n               mif2() -> mf\n             ll <- logmeanexp(replicate(5,logLik(pfilter(mf))),se=TRUE)\n             data.frame(loglik=ll[1],loglik.se=ll[2],as.list(coef(mf)))\n           }\n}) -> mles\n```\nA useful way to view the data is with a pairwise scatterplot matrix:\n```{r matrix2}\npairs(~loglik+beta_sd+Beta+mu+gamma+rho+N,data=mles)\n```\n\nFrom looking at the matrix, there does not seem to be any linear correlations between the parameters in this model. Most of the paramters seem to gravitate near one location for the best log likelihood. \n\nTo get better estimates and obtain likelihood-ratio-test based confidence intervals, I've constructed profile likelihoods for two important parameters: beta (transmission term) and gamma (recovery rate). In a likelihood profile, one varies the focal parameter across some range, maximizing the likelihood at each value of the focal parameter over the remaining parameters. \n\n####Cumulative Beta Profile \n\nThe beta profile is constructed below. \n```{r betaCum profile}\nprofileDesign(\n  Beta=seq(from=1.0, to=3.0, length=20),\n  lower=c(beta_sd=0.05, mu=0.01, gamma=0.1, rho=0.05, N=10000),\n  upper=c(beta_sd=0.5, mu=1.0, gamma=0.99, rho=0.99, N=10000),\n  nprof=50\n) -> cumBetaPD\n\nbake(\"CumFakedata_Beta-profile.rds\",{\n  foreach (p=iter(cumBetaPD,\"row\"),\n           .combine=rbind,\n           .errorhandling=\"remove\",\n           .packages=c(\"pomp\",\"magrittr\",\"reshape2\",\"plyr\"),\n           .inorder=FALSE,\n           .options.mpi=list(seed=1680158025)\n  ) %dopar% {\n    sample %>% \n      mif2(start=unlist(p),Nmif=50,Np=1000,transform=TRUE,\n           cooling.fraction.50=0.8,cooling.type=\"geometric\",\n           rw.sd=rw.sd(beta_sd=0.02, mu=0.02, gamma=0.02, rho=0.02, N=0.02)) %>%\n      mif2() -> mf\n    \n    pf <- replicate(5,pfilter(mf,Np=1000))  ## independent particle filters\n    ll <- sapply(pf,logLik)\n    ll <- logmeanexp(ll,se=TRUE)\n    nfail <- sapply(pf,getElement,\"nfail\")  ## number of filtering failures\n    \n    data.frame(as.list(coef(mf)),\n               loglik = ll[1],\n               loglik.se = ll[2],\n               nfail.min = min(nfail),\n               nfail.max = max(nfail))\n  } %>% arrange(Beta,-loglik)\n}) -> beta_prof_cum\n```\n\nPairwise scatterplot matrix for Beta: \n```{r betaSM2}\npairs(~loglik+Beta+beta_sd+mu+gamma+rho+N,data=beta_prof_cum,subset=loglik>max(loglik)-10)\n```\n\nThere appears to be a positive linear correlation between Beta and mu.\n\n```{r betaCIProfileGraphCum}\nbeta_prof_cum %>% \n  mutate(Beta=signif(Beta,8)) %>%\n  ddply(~Beta,subset,loglik==max(loglik)) %>%\n  ggplot(aes(x=Beta,y=loglik))+geom_point()+geom_smooth()\n```\n\nThis graph shows Beta plotted as a function of log likeihood. The maximum log likelihood appears to be close to 1.7, with the actual value being 1.5. The gray ribbon surrounding the blue line represents the confidence interval estimate, and it is seen that 80% of the points actually fall within the 95% confidence interval. \n\nNow let us plot the data and 10 simulated realizations of the model process on the same axes.\n\n```{r betaProfileFitCum}\nbeta_prof_cum %>% \n  subset(loglik==max(loglik)) %>% unlist() -> mle\nsimulate(cumulativeModel,params=mle,nsim=10,as.data.frame=TRUE,include.data=TRUE) %>%\n  ggplot(mapping=aes(x=time,y=Cases,group=sim,alpha=(sim==\"data\")))+\n  scale_alpha_manual(name=\"\",values=c(`TRUE`=1,`FALSE`=0.2),\n                     labels=c(`FALSE`=\"simulation\",`TRUE`=\"data\"))+\n  geom_line()\n```\n\n####Cumulative Gamma Profile\nThe gamma profile is constructed below. \n```{r gammaCum profile}\nprofileDesign(\n  gamma=seq(from=0.1, to=1.0, length=20),\n  lower=c(beta_sd=0.05, mu=0.01, Beta=1.0, rho=0.05, N=10000),\n  upper=c(beta_sd=0.5, mu=1.0, Beta=3.0, rho=0.99, N=10000),\n  nprof=50\n) -> cumGammaPD\n\nbake(\"CumFakedata_gamma-profile.rds\",{\n  foreach (p=iter(cumGammaPD,\"row\"),\n           .combine=rbind,\n           .errorhandling=\"remove\",\n           .packages=c(\"pomp\",\"magrittr\",\"reshape2\",\"plyr\"),\n           .inorder=FALSE,\n           .options.mpi=list(seed=1680158025)\n  ) %dopar% {\n    sample %>% \n      mif2(start=unlist(p),Nmif=50,Np=1000,transform=TRUE,\n           cooling.fraction.50=0.8,cooling.type=\"geometric\",\n           rw.sd=rw.sd(beta_sd=0.02, mu=0.02, Beta=0.02, rho=0.02, N=0.02)) %>%\n      mif2() -> mf\n    \n    pf <- replicate(5,pfilter(mf,Np=1000))  ## independent particle filters\n    ll <- sapply(pf,logLik)\n    ll <- logmeanexp(ll,se=TRUE)\n    nfail <- sapply(pf,getElement,\"nfail\")  ## number of filtering failures\n    \n    data.frame(as.list(coef(mf)),\n               loglik = ll[1],\n               loglik.se = ll[2],\n               nfail.min = min(nfail),\n               nfail.max = max(nfail))\n  } %>% arrange(gamma,-loglik)\n}) -> gamma_prof_cum\n```\n\nPairwise scatterplot matrix for Gamma: \n```{r gammaSM2}\npairs(~loglik+gamma+beta_sd+mu+Beta+rho+N,data=gamma_prof_cum,subset=loglik>max(loglik)-10)\n```\n\nGamma does not appear to have any linear correlation with any other parameter. \n\n```{r gammaCIProfileGraphCum}\ngamma_prof_cum %>% \n  mutate(gamma=signif(gamma,8)) %>%\n  ddply(~gamma,subset,loglik==max(loglik)) %>%\n  ggplot(aes(x=gamma,y=loglik))+geom_point()+geom_smooth()\n```\n\nThis graph shows gamma plotted as a function of log likeihood. The maximum log likelihood appears to be close to .15, with 0.6 being its true parameter value. The gray ribbon surrounding the blue line represents the confidence interval estimate, and it is seen that 70% of the points actually fall within the 95% confidence interval. Likely lower coverage due to more variability.  \n\nNow let us plot the data and 10 simulated realizations of the model process on the same axes.\n```{r gammaProfileFitCum}\ngamma_prof_cum %>% \n  subset(loglik==max(loglik)) %>% unlist() -> mle\nsimulate(cumulativeModel,params=mle,nsim=10,as.data.frame=TRUE,include.data=TRUE) %>%\n  ggplot(mapping=aes(x=time,y=Cases,group=sim,alpha=(sim==\"data\")))+\n  scale_alpha_manual(name=\"\",values=c(`TRUE`=1,`FALSE`=0.2),\n                     labels=c(`FALSE`=\"simulation\",`TRUE`=\"data\"))+\n  geom_line()\n```\n\n##Discussion\n\nIt appears that even when one includes the entire time-series of an epidemic, not just the takeoff, that fitting deterministic models to cumulative data can lead to underestimation of uncertainity in parameter estimates. It also seems to fail to fit closely to the data in many instances. When one considers the results of the iterated filtering algorithms, it appears that despite the fact that cumulative cases seem to have more achieved coverage in its parameter estiamtes for Beta and gamma, it also fails to accurately predict the paramter esitmates at the maximum log likelihood. It also still struggles to fit to the data accurately, whereas the Beta and gamma profile fits for the raw data do incredibly well, with an achieved confidence interval coverage close to the cumulative data. It would be beneficial to perhaps run more iterated filtering algorithms on more than just one dataset to see if the results can be repeated. However, this is a computaionally expensive operation. \n\n##References\n\n",
    "created" : 1467744453777.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "120203951",
    "id" : "309999C3",
    "lastKnownWriteTime" : 1467745708,
    "path" : "~/R/Fake_Data/Fake_Data/Simulated Data Work.Rmd",
    "project_path" : "Simulated Data Work.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}